#include <linux/uSnap.h>
#include <linux/mm.h>
#include <linux/module.h>
#include <linux/sched.h>
#include <linux/rwsem.h>
#include <linux/list.h>
#include <linux/spinlock.h>
#include <linux/mm_types.h>
#include <linux/mmu_notifier.h>
#include <linux/user_namespace.h>
#include <linux/uprobes.h>
#include <linux/ksm.h>
#include <linux/khugepaged.h>
#include <linux/security.h>
#include <linux/mempolicy.h>
#include <linux/rmap.h>
#include <linux/fs.h>
#include <linux/fs_struct.h>
#include <linux/hugetlb.h>
#include <linux/fdtable.h>
#include <linux/mman.h>
#include <linux/binfmts.h>
#include <linux/ioprio.h>
#include <linux/tty.h>
#include <linux/vmacache.h>
#include <linux/user-return-notifier.h>
#include <linux/vmalloc.h>
#include <linux/ftrace.h>
#include <linux/delayacct.h>
#include <linux/task_io_accounting_ops.h>
#include <linux/tsacct_kern.h>
#include <linux/perf_event.h>
#include <linux/audit.h>
#include <linux/random.h>
#include <linux/cn_proc.h>
#include <linux/delay.h>


#include <asm/mmu_context.h>

static __latent_entropy int uSnap_dup_mmap(struct mm_struct *mm,
					struct mm_struct *oldmm)
{
	struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
	struct rb_node **rb_link, *rb_parent;
	int retval;
	unsigned long charge;

	uprobe_start_dup_mmap();
	if (down_write_killable(&oldmm->mmap_sem)) {
		retval = -EINTR;
		goto fail_uprobe_end;
	}
	flush_cache_dup_mm(oldmm);
	uprobe_dup_mmap(oldmm, mm);
	/*
	 * Not linked in yet - no deadlock potential:
	 */
	down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);

	/* No ordering required: file already has been exposed. */
	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));

	mm->total_vm = oldmm->total_vm;
	mm->data_vm = oldmm->data_vm;
	mm->exec_vm = oldmm->exec_vm;
	mm->stack_vm = oldmm->stack_vm;

	rb_link = &mm->mm_rb.rb_node;
	rb_parent = NULL;
	pprev = &mm->mmap;
	retval = ksm_fork(mm, oldmm);
	if (retval)
		goto out;
	retval = khugepaged_fork(mm, oldmm);
	if (retval)
		goto out;

	prev = NULL;
	for (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {
		struct file *file;

		if (mpnt->vm_flags & VM_DONTCOPY) {
			vm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));
			continue;
		}
		charge = 0;
		if (mpnt->vm_flags & VM_ACCOUNT) {
			unsigned long len = vma_pages(mpnt);

			if (security_vm_enough_memory_mm(oldmm, len)) /* sic */
				goto fail_nomem;
			charge = len;
		}

		tmp = &uSnap_kern->vm_area_struct;
	
		*tmp = *mpnt;

		// LIST
		INIT_LIST_HEAD(&tmp->anon_vma_chain);
		retval = vma_dup_policy(mpnt, tmp);
		if (retval)
			goto fail_nomem_policy;
		tmp->vm_mm = mm;

		if (uSnap_anon_vma_fork(tmp, mpnt))
			goto fail_nomem_anon_vma_fork;
		tmp->vm_flags &=
			~(VM_LOCKED|VM_LOCKONFAULT|VM_UFFD_MISSING|VM_UFFD_WP);
		tmp->vm_next = tmp->vm_prev = NULL;
		tmp->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
		file = tmp->vm_file;
		if (file) {
			struct inode *inode = file_inode(file);
			struct address_space *mapping = file->f_mapping;

			get_file(file);
			if (tmp->vm_flags & VM_DENYWRITE)
				atomic_dec(&inode->i_writecount);
			i_mmap_lock_write(mapping);
			if (tmp->vm_flags & VM_SHARED)
				atomic_inc(&mapping->i_mmap_writable);
			flush_dcache_mmap_lock(mapping);
			/* insert tmp into the share list, just after mpnt */
			vma_interval_tree_insert_after(tmp, mpnt,
					&mapping->i_mmap);
			flush_dcache_mmap_unlock(mapping);
			i_mmap_unlock_write(mapping);
		}

		/*
		 * Clear hugetlb-related page reserves for children. This only
		 * affects MAP_PRIVATE mappings. Faults generated by the child
		 * are not guaranteed to succeed, even if read-only
		 */
		if (is_vm_hugetlb_page(tmp))
			reset_vma_resv_huge_pages(tmp);

		/*
		 * Link in the new vma and copy the page table entries.
		 */
		*pprev = tmp;
		pprev = &tmp->vm_next;
		tmp->vm_prev = prev;
		prev = tmp;

		__vma_link_rb(mm, tmp, rb_link, rb_parent);
		rb_link = &tmp->vm_rb.rb_right;
		rb_parent = &tmp->vm_rb;

		mm->map_count++;
		// delete later
		retval = copy_page_range(mm, oldmm, mpnt);

		if (tmp->vm_ops && tmp->vm_ops->open)
			tmp->vm_ops->open(tmp);

		if (retval)
			goto out;
	}
	/* a new mm has just been created */
	// delete later
	arch_dup_mmap(oldmm, mm);
	retval = 0;
out:
	up_write(&mm->mmap_sem);
	flush_tlb_mm(oldmm);
	up_write(&oldmm->mmap_sem);
fail_uprobe_end:
	uprobe_end_dup_mmap();
	return retval;
fail_nomem_anon_vma_fork:
	mpol_put(vma_policy(tmp));
fail_nomem_policy:
fail_nomem:
	retval = -ENOMEM;
	vm_unacct_memory(charge);
	goto out;
} //done

static void uSnap_mm_init_aio(struct mm_struct *mm)
{
#ifdef CONFIG_AIO
	spin_lock_init(&mm->ioctx_lock);
	mm->ioctx_table = NULL;
#endif
} //done

static void uSnap_mm_init_owner(struct mm_struct *mm, struct task_struct *p)
{
#ifdef CONFIG_MEMCG
	mm->owner = p;
#endif
} //done

static inline int uSnap_mm_alloc_pgd(struct mm_struct *mm)
{
	mm->pgd = pgd_alloc(mm);
	if (unlikely(!mm->pgd))
		return -ENOMEM;
	return 0;
} //done

static inline void uSnap_mm_free_pgd(struct mm_struct *mm)
{
	pgd_free(mm, mm->pgd);
} //done

static struct mm_struct *uSnap_mm_init(struct mm_struct *mm, struct task_struct *p,
	struct user_namespace *user_ns, struct task_struct *to_copy)
{
	mm->mmap = NULL;
	mm->mm_rb = RB_ROOT;
	mm->vmacache_seqnum = 0;
	atomic_set(&mm->mm_users, 1);
	atomic_set(&mm->mm_count, 1);
	init_rwsem(&mm->mmap_sem);
	INIT_LIST_HEAD(&mm->mmlist);
	mm->core_state = NULL;
	atomic_long_set(&mm->nr_ptes, 0);
	mm_nr_pmds_init(mm);
	mm->map_count = 0;
	mm->locked_vm = 0;
	mm->pinned_vm = 0;
	memset(&mm->rss_stat, 0, sizeof(mm->rss_stat));
	spin_lock_init(&mm->page_table_lock);
	mm_init_cpumask(mm);
	uSnap_mm_init_aio(mm);
	uSnap_mm_init_owner(mm, p);
	mmu_notifier_mm_init(mm);
	clear_tlb_flush_pending(mm);
#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
	mm->pmd_huge_pte = NULL;
#endif

	if (to_copy->mm) {
		mm->flags = to_copy->mm->flags & MMF_INIT_MASK;
		mm->def_flags = to_copy->mm->def_flags & VM_INIT_DEF_MASK;
	} else {
		mm->flags = MMF_DUMP_FILTER_DEFAULT;
		mm->def_flags = 0;
	}

	// delete later
	if (uSnap_mm_alloc_pgd(mm))
		goto fail_nopgd;

	// think later.. current involved
	if (init_new_context(p, mm))
		goto fail_nocontext;

	mm->user_ns = get_user_ns(user_ns);
	return mm;

fail_nocontext:
	uSnap_mm_free_pgd(mm);
fail_nopgd:
	return NULL;
} //done

static struct mm_struct *uSnap_dup_mm(struct task_struct *tsk, struct task_struct *to_copy)
{
	struct mm_struct *mm, *oldmm = to_copy->mm;
	int err;

	mm = &uSnap_kern->mm;

	memcpy(mm, oldmm, sizeof(*mm));

	if (!uSnap_mm_init(mm, tsk, mm->user_ns, to_copy))
		goto fail_nomem;

	err = uSnap_dup_mmap(mm, oldmm);
	if (err)
		goto free_pt;

	mm->hiwater_rss = get_mm_rss(mm);
	mm->hiwater_vm = mm->total_vm;

	if (mm->binfmt && !try_module_get(mm->binfmt->module))
		goto free_pt;

	return mm;

free_pt:
	mm->binfmt = NULL;
	mmput(mm);

fail_nomem:
	return NULL;
} //done

static int uSnap_copy_mm(struct task_struct *tsk, struct task_struct *to_copy)
{
	struct mm_struct *mm, *oldmm;
	int retval;

	tsk->min_flt = tsk->maj_flt = 0;
	tsk->nvcsw = tsk->nivcsw = 0;
#ifdef CONFIG_DETECT_HUNG_TASK
	tsk->last_switch_count = tsk->nvcsw + tsk->nivcsw;
#endif

	tsk->mm = NULL;
	tsk->active_mm = NULL;

	oldmm = to_copy->mm;
	if (!oldmm)
		return 0;


	vmacache_flush(tsk);



	retval = -ENOMEM;
	mm = uSnap_dup_mm(tsk, to_copy);
	if (!mm)
		goto fail_nomem;

	tsk->mm = mm;
	tsk->active_mm = mm;
	return 0;

fail_nomem:
	return retval;
}

static struct fs_struct *uSnap_copy_fs_struct(struct fs_struct *old)
{
	struct fs_struct *fs = &uSnap_kern->fs_struct;

	if (fs) {
		fs->users = 1;
		fs->in_exec = 0;
		spin_lock_init(&fs->lock);
		seqcount_init(&fs->seq);
		fs->umask = old->umask;

		spin_lock(&old->lock);
		fs->root = old->root;
		path_get(&fs->root);
		fs->pwd = old->pwd;
		path_get(&fs->pwd);
		spin_unlock(&old->lock);
	}
	return fs;
}//done


static int uSnap_copy_fs(struct task_struct *tsk, struct task_struct *to_copy)
{
	struct fs_struct *fs = to_copy->fs;
	
	tsk->fs = uSnap_copy_fs_struct(fs);
	if (!tsk->fs)
		return -ENOMEM;
	return 0;
}//done

static int uSnap_copy_files(struct task_struct *tsk, struct task_struct *to_copy)
{
	struct files_struct *oldf, *newf;
	int error = 0;

	oldf = to_copy->files;
	if (!oldf)
		goto out;

	//visit later,,, is fd only one dynamically allocated?
	newf = dup_fd(oldf, &error);
	if (!newf)
		goto out;

	tsk->files = newf;
	error = 0;
out:
	return error;
}

static int uSnap_copy_io(struct task_struct *tsk, struct task_struct *to_copy)
{
#ifdef CONFIG_BLOCK
	struct io_context *ioc = to_copy->io_context;
	struct io_context *new_ioc;

	if (!ioc)
		return 0;

	if (ioprio_valid(ioc->ioprio)) {
		new_ioc = uSnap_get_task_io_context(tsk, GFP_KERNEL, NUMA_NO_NODE);
		if (unlikely(!new_ioc))
			return -ENOMEM;

		new_ioc->ioprio = ioc->ioprio;
		put_io_context(new_ioc);
	}
#endif
	return 0;
} // half done

static int uSnap_copy_sighand(struct task_struct *tsk, struct task_struct *to_copy)
{
	struct sighand_struct *sig;

	sig = &uSnap_kern->sighand_struct;
	rcu_assign_pointer(tsk->sighand, sig);

	atomic_set(&sig->count, 1);
	memcpy(sig->action, to_copy->sighand->action, sizeof(sig->action));
	return 0;
}//done

static void uSnap_posix_cpu_timers_init_group(struct signal_struct *sig)
{
	unsigned long cpu_limit;

	cpu_limit = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
	if (cpu_limit != RLIM_INFINITY) {
		sig->cputime_expires.prof_exp = secs_to_cputime(cpu_limit);
		sig->cputimer.running = true;
	}

	/* The timer lists. */
	INIT_LIST_HEAD(&sig->cpu_timers[0]);
	INIT_LIST_HEAD(&sig->cpu_timers[1]);
	INIT_LIST_HEAD(&sig->cpu_timers[2]);
}

static int uSnap_copy_signal(struct task_struct *tsk, struct task_struct *to_copy)
{
	struct signal_struct *sig;

	sig = &uSnap_kern->signal_struct;

	tsk->signal = sig;

	sig->nr_threads = 1;
	atomic_set(&sig->live, 1);
	atomic_set(&sig->sigcnt, 1);

	/* list_add(thread_node, thread_head) without INIT_LIST_HEAD() */
	sig->thread_head = (struct list_head)LIST_HEAD_INIT(tsk->thread_node);
	tsk->thread_node = (struct list_head)LIST_HEAD_INIT(sig->thread_head);

	init_waitqueue_head(&sig->wait_chldexit);
	sig->curr_target = tsk;
	init_sigpending(&sig->shared_pending);
	INIT_LIST_HEAD(&sig->posix_timers);
	seqlock_init(&sig->stats_lock);
	prev_cputime_init(&sig->prev_cputime);

	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
	sig->real_timer.function = it_real_fn;

	task_lock(to_copy->group_leader);
	memcpy(sig->rlim, to_copy->signal->rlim, sizeof sig->rlim);
	task_unlock(to_copy->group_leader);

	uSnap_posix_cpu_timers_init_group(sig);

	tty_audit_fork(sig);
	sched_autogroup_fork(sig);

	sig->oom_score_adj = to_copy->signal->oom_score_adj;
	sig->oom_score_adj_min = to_copy->signal->oom_score_adj_min;

	sig->has_child_subreaper = to_copy->signal->has_child_subreaper ||
				   to_copy->signal->is_child_subreaper;

	mutex_init(&sig->cred_guard_mutex);

	return 0;
} //done

static void uSnap_copy_seccomp(struct task_struct *p, struct task_struct *to_copy)
{
#ifdef CONFIG_SECCOMP
	assert_spin_locked(&to_copy->sighand->siglock);

	get_seccomp_filter(to_copy);
	p->seccomp = to_copy->seccomp;

	if (task_no_new_privs(to_copy))
		task_set_no_new_privs(p);

	if (p->seccomp.mode != SECCOMP_MODE_DISABLED)
		set_tsk_thread_flag(p, TIF_SECCOMP);
#endif
}

static void uSnap_account_kernel_stack(struct task_struct *tsk, int account)
{
	void *stack = task_stack_page(tsk);
	struct vm_struct *vm = task_stack_vm_area(tsk);

	BUILD_BUG_ON(IS_ENABLED(CONFIG_VMAP_STACK) && PAGE_SIZE % 1024 != 0);

	if (vm) {
		int i;

		BUG_ON(vm->nr_pages != THREAD_SIZE / PAGE_SIZE);

		for (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++) {
			mod_zone_page_state(page_zone(vm->pages[i]),
					    NR_KERNEL_STACK_KB,
					    PAGE_SIZE / 1024 * account);
		}

		/* All stack pages belong to the same memcg. */
		memcg_kmem_update_page_stat(vm->pages[0], MEMCG_KERNEL_STACK_KB,
					    account * (THREAD_SIZE / 1024));
	} else {
		/*
		 * All stack pages are in the same zone and belong to the
		 * same memcg.
		 */
		struct page *first_page = virt_to_page(stack);

		mod_zone_page_state(page_zone(first_page), NR_KERNEL_STACK_KB,
				    THREAD_SIZE / 1024 * account);

		memcg_kmem_update_page_stat(first_page, MEMCG_KERNEL_STACK_KB,
					    account * (THREAD_SIZE / 1024));
	}
}


static struct task_struct *uSnap_dup_task_struct(struct task_struct *orig, int node)
{
	struct task_struct *tsk;
	unsigned long *stack;
	struct vm_struct *stack_vm_area;
	int err;


	tsk = &uSnap_kern->task_struct;

	stack = (unsigned long*)uSnap_kern->stack;

	stack_vm_area = task_stack_vm_area(tsk);

	err = arch_dup_task_struct(tsk, orig);

	/*
	 * arch_dup_task_struct() clobbers the stack-related fields.  Make
	 * sure they're properly initialized before using any stack-related
	 * functions again.
	 */
	tsk->stack = stack;
#ifdef CONFIG_VMAP_STACK
	tsk->stack_vm_area = stack_vm_area;
#endif
#ifdef CONFIG_THREAD_INFO_IN_TASK
	atomic_set(&tsk->stack_refcount, 1);
#endif

	if (err)
		return NULL;

#ifdef CONFIG_SECCOMP
	/*
	 * We must handle setting up seccomp filters once we're under
	 * the sighand lock in case orig has changed between now and
	 * then. Until then, filter must be NULL to avoid messing up
	 * the usage counts on the error path calling free_task.
	 */
	tsk->seccomp.filter = NULL;
#endif

	setup_thread_stack(tsk, orig);
	clear_user_return_notifier(tsk);
	clear_tsk_need_resched(tsk);
	set_task_stack_end_magic(tsk);

#ifdef CONFIG_CC_STACKPROTECTOR
	tsk->stack_canary = get_random_int();
#endif

	/*
	 * One for us, one for whoever does the "release_task()" (usually
	 * parent)
	 */
	atomic_set(&tsk->usage, 2);
#ifdef CONFIG_BLK_DEV_IO_TRACE
	tsk->btrace_seq = 0;
#endif
	tsk->splice_pipe = NULL;
	tsk->task_frag.page = NULL;
	tsk->wake_q.next = NULL;

//	uSnap_account_kernel_stack(tsk, 1);

	kcov_task_init(tsk);

	return tsk;
} //done

static void uSnap_rt_mutex_init_task(struct task_struct *p)
{
	raw_spin_lock_init(&p->pi_lock);
#ifdef CONFIG_RT_MUTEXES
	p->pi_waiters = RB_ROOT;
	p->pi_waiters_leftmost = NULL;
	p->pi_blocked_on = NULL;
#endif
}

static void uSnap_posix_cpu_timers_init(struct task_struct *tsk)
{
	tsk->cputime_expires.prof_exp = 0;
	tsk->cputime_expires.virt_exp = 0;
	tsk->cputime_expires.sched_exp = 0;
	INIT_LIST_HEAD(&tsk->cpu_timers[0]);
	INIT_LIST_HEAD(&tsk->cpu_timers[1]);
	INIT_LIST_HEAD(&tsk->cpu_timers[2]);
}

static inline void
uSnap_init_task_pid(struct task_struct *task, enum pid_type type, struct pid *pid)
{
	 task->pids[type].pid = pid;
}

struct task_struct* uSnap_dup_task(struct task_struct *to_copy)
{
	struct task_struct *new;
	struct pid* pid;
	int ret;
	unsigned int cpu = smp_processor_id();

	printk(KERN_ALERT"dup_task_struct\n");
	new = uSnap_dup_task_struct(to_copy, cpu_to_node(cpu));

	if(!new)
	{
		printk(KERN_ALERT"uSnap] Dup task Error\n");
		return NULL;
	}

	ftrace_graph_init_task(new);
	uSnap_rt_mutex_init_task(new);

#ifdef CONFIG_PROVE_LOCKING
	DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);
	DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
#endif

	printk(KERN_ALERT"copy_creds\n");
	ret = uSnap_copy_creds(new, to_copy);

	if( ret < 0 )
	{
		printk(KERN_ALERT"uSnap] Copy creds Error\n");
		return NULL;
	}

	delayacct_tsk_init(new);
	new->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);
	new->flags |= PF_FORKNOEXEC;

	INIT_LIST_HEAD(&new->children);
	INIT_LIST_HEAD(&new->sibling);
	rcu_copy_process(new);
	new->vfork_done = NULL;
	spin_lock_init( &new->alloc_lock);
	init_sigpending(&new->pending);
	
	new->utime = new->stime = new->gtime = 0;
	new->utimescaled = new->stimescaled = 0;
	prev_cputime_init(&new->prev_cputime);

#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
	seqcount_init(&new->vtime_seqcount);
	new->vtime_snap = 0;
	new->vtime_snap_whence = VTIME_INACTIVE;
#endif

#if defined(SPLIT_RSS_COUNTING)
	memset(&new->rss_stat, 0, sizeof(new->rss_stat));
#endif
	new->default_timer_slack_ns = to_copy->timer_slack_ns;
	task_io_accounting_init( &new->ioac );
	acct_clear_integrals(new);
	uSnap_posix_cpu_timers_init(new);

	new->start_time = ktime_get_ns();
	new->real_start_time = ktime_get_boot_ns();
	new->io_context = NULL;
	new->audit_context = NULL;
	cgroup_fork(new);

#ifdef CONFIG_NUMA
	printk(KERN_ALERT"mpol_dup\n");
	new->mempolicy = uSnap_mpol_dup(new->mempolicy, to_copy);

	if( IS_ERR(new->mempolicy) )
	{
		printk(KERN_ALERT"uSnap] mpol dup Error\n");
		return NULL;
	}
#endif
#ifdef CONFIG_CPUSETS
	new->cpuset_mem_spread_rotor = NUMA_NO_NODE;
	new->cpuset_slab_spread_rotor = NUMA_NO_NODE;
	seqcount_init(&new->mems_allowed_seq);
#endif

#ifdef CONFIG_TRACE_IRQFLAGS
	new->irq_events = 0;

	new->hardirqs_enabled = 0;
	new->hardirq_enable_ip = 0;
	new->hardirq_enable_event = 0;
	new->hardirq_disable_ip = _THIS_IP_;
	new->hardirq_disable_event = 0;

	new->softirqs_enabled = 1;
	new->softirq_enable+ip = _THIS_IP_;
	new->softirq_enable_event = 0;
	new->softirq_disable_ip = 0;
	new->softirq_disable_event = 0;

	new->hardirq_context = 0;
	new->softirq_context = 0;
#endif

	new->pagefault_disabled = 0;

#ifdef CONFIG_LOCKDEP
	new->lockdep_depth = 0;
	new->curr_chain_key = 0;
	new->lockdep_recursion = 0;
#endif

#ifdef CONFIG_DEBUG_MUTEXES
	new->blocked_on = NULL;
#endif

#ifdef CONFIG_BCACHE
	new->sequential_io = 0;
	new->sequential_io_avg = 0;
#endif

	printk(KERN_ALERT"sched_fork\n");
	ret = sched_fork(0, new);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] sched fork Error\n");
		return NULL;
	}

	
	printk(KERN_ALERT"perf_event_init_task\n");
	ret = perf_event_init_task(new);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] perf event init Error\n");
		return NULL;
	}
	

	printk(KERN_ALERT"audit_alloc\n");
	ret = audit_alloc(new);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] audit alloc Error\n");
		return NULL;
	}

	printk(KERN_ALERT"shm_init_task\n");
	shm_init_task(new);

	printk(KERN_ALERT"copy_semundo\n");
	ret = uSnap_copy_semundo(new, to_copy);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] copy semundo Error\n");
		return NULL;
	}

	printk(KERN_ALERT"copy_files\n");
	ret = uSnap_copy_files(new, to_copy);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] copy files Error\n");
		return NULL;
	}

	printk(KERN_ALERT"copy_fs\n");
	ret = uSnap_copy_fs(new, to_copy);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] copy fs Error\n");
		return NULL;
	}

	printk(KERN_ALERT"copy_sighand\n");
	ret = uSnap_copy_sighand( new, to_copy);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] copy sighand Error\n");
		return NULL;
	}

	printk(KERN_ALERT"copy_signal\n");
	ret = uSnap_copy_signal(new, to_copy);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] copy signal ERror\n");
		return NULL;
	}

	printk(KERN_ALERT"copy_mm\n");
	ret = uSnap_copy_mm(new, to_copy);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] copy mm Error\n");
		return NULL;
	}

	printk(KERN_ALERT"copy_namespaces\n");
	ret = uSnap_copy_namespaces(new, to_copy);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] copy namespaces Error\n");
		return NULL;
	}

	printk(KERN_ALERT"copy_io\n");
	ret = uSnap_copy_io(new, to_copy);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] copy io Error\n");
		return NULL;
	}

	printk(KERN_ALERT"copy_thread_tls\n");
	ret = uSnap_copy_thread_tls(0,0, new,0, to_copy);

	if( ret )
	{
		printk(KERN_ALERT"uSnap] copy thread tls Error\n");
		return NULL;
	}

	printk(KERN_ALERT"alloc_pid\n");
	pid = alloc_pid(new->nsproxy->pid_ns_for_children);

	if( IS_ERR(pid) )
	{
		printk(KERN_ALERT"uSnap] alloc pid Error\n");
		return NULL;
	}

	new->set_child_tid = NULL;
	new->clear_child_tid = NULL;

#ifdef CONFIG_BLOCK
	new->plug = NULL;
#endif

#ifdef CONFIG_FUTEX
	new->robust_list = NULL;
#ifdef CONFIG_COMPAT
	new->compat_robust_list = NULL;
#endif
	INIT_LIST_HEAD(&new->pi_state_list);
	new->pi_state_cache = NULL;
#endif
	user_disable_single_step(new);
	clear_tsk_thread_flag(new, TIF_SYSCALL_TRACE);
#ifdef TIF_SYSCALL_EMU
	clear_tsk_thread_flag(new, TIF_SYSCALL_EMU);
#endif
	clear_all_latency_tracing(new);

	new->pid = pid_nr(pid);
	new->exit_signal = to_copy->group_leader->exit_signal;

	new->group_leader = new;
	new->tgid = new->pid;

	new->nr_dirtied = 0;
	new->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);
	new->dirty_paused_when = 0;

	new->pdeath_signal = 0;
	INIT_LIST_HEAD(&new->thread_group);
	new->task_works = NULL;

	threadgroup_change_begin(to_copy);

	printk(KERN_ALERT"cgroup_can_fork\n");
	ret = cgroup_can_fork(new);
	if( ret )
	{
		printk(KERN_ALERT"cgroup can fork Error\n");
		return NULL;
	}
	write_lock_irq(&tasklist_lock);

	new->real_parent = to_copy->real_parent;
	new->parent_exec_id = to_copy->parent_exec_id;

	spin_lock(&to_copy->sighand->siglock);

	printk(KERN_ALERT"copy_seccomp\n");
	uSnap_copy_seccomp(new, to_copy);

	recalc_sigpending();

	if( signal_pending(to_copy) )
	{
		spin_unlock(&to_copy->sighand->siglock);
		write_unlock_irq(&tasklist_lock);
		printk(KERN_ALERT"signal pending\n");
		return NULL;
	}

	printk(KERN_ALERT"init_task_pid\n");
	if( new->pid )
	{
		uSnap_init_task_pid( new, PIDTYPE_PID, pid);
		if(thread_group_leader(new))
		{
			uSnap_init_task_pid(new, PIDTYPE_PGID, task_pgrp(to_copy));
			uSnap_init_task_pid(new, PIDTYPE_SID, task_session(to_copy));

			if( is_child_reaper(pid))
			{
				ns_of_pid(pid)->child_reaper = new;
				new->signal->flags |= SIGNAL_UNKILLABLE;
			}

			new->signal->leader_pid = pid;
			new->signal->tty = tty_kref_get(to_copy->signal->tty);
			list_add_tail(&new->sibling, &new->real_parent->children);
			list_add_tail_rcu(&new->tasks, &init_task.tasks);

			attach_pid(new, PIDTYPE_PGID);
			attach_pid(new, PIDTYPE_SID);

			__this_cpu_inc(process_counts);
		}
		else
		{
			to_copy->signal->nr_threads++;
			atomic_inc(&to_copy->signal->live);
			atomic_inc(&to_copy->signal->sigcnt);
			list_add_tail_rcu(&new->thread_group, &new->group_leader->thread_group);
			list_add_tail_rcu(&new->thread_node, &new->signal->thread_head);
		}
		attach_pid(new, PIDTYPE_PID);
		nr_threads++;
	}

	total_forks++;
	spin_unlock(&to_copy->sighand->siglock);
	
	write_unlock_irq(&tasklist_lock);
	proc_fork_connector(new);
	cgroup_post_fork(new);
	threadgroup_change_end(to_copy);
	
	add_latent_entropy();

	if(!IS_ERR(new))
	{
		struct pid *pid;

		pid = get_task_pid(new, PIDTYPE_PID);
		//wake_up_new_task(new);
		put_pid(pid);
	}
	return new;
}

